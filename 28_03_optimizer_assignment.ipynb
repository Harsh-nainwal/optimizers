{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3d2ce5-fece-4e3c-886a-28c62d4f095d",
   "metadata": {},
   "source": [
    "**Part 1: Understanding Optimizers**\n",
    "\n",
    "1. **Role of Optimization Algorithms:**\n",
    "   Optimization algorithms in artificial neural networks play a crucial role in minimizing the loss function during the training process. They adjust the parameters (weights and biases) of the neural network to optimize its performance. Without optimization algorithms, the network would not be able to learn from the data and improve its predictive capabilities.\n",
    "\n",
    "2. **Gradient Descent and its Variants:**\n",
    "   Gradient descent is an iterative optimization algorithm used to minimize the loss function by adjusting the parameters of the model in the direction of the steepest descent of the gradient. Variants of gradient descent include:\n",
    "   - **Batch Gradient Descent**: Computes the gradient of the loss function with respect to the entire training dataset.\n",
    "   - **Stochastic Gradient Descent (SGD)**: Updates the parameters using the gradient of the loss function with respect to a single training example.\n",
    "   - **Mini-batch Gradient Descent**: Updates the parameters using the gradient computed over a small subset of the training data.\n",
    "\n",
    "   Each variant has tradeoffs in terms of convergence speed and memory requirements. SGD and mini-batch gradient descent are often preferred over batch gradient descent due to their faster convergence and lower memory requirements, especially for large datasets.\n",
    "\n",
    "3. **Challenges of Traditional Gradient Descent:**\n",
    "   Traditional gradient descent methods may face challenges such as slow convergence and getting stuck in local minima. These issues can significantly hinder the training process, especially for deep neural networks with complex loss surfaces.\n",
    "\n",
    "4. **Modern Optimizers to Address Challenges:**\n",
    "   Modern optimizers address the challenges of traditional gradient descent by introducing adaptive learning rates, momentum, and other techniques. These optimizers, such as Adam, RMSprop, and others, adaptively adjust the learning rate based on the gradients of the parameters and previous updates, allowing for faster convergence and improved performance.\n",
    "\n",
    "5. **Momentum and Learning Rate:**\n",
    "   Momentum in optimization algorithms introduces inertia by accumulating gradients from previous steps, which helps overcome local minima and accelerate convergence. Learning rate determines the step size during parameter updates. A higher learning rate can lead to faster convergence but may risk overshooting the optimal solution, while a lower learning rate may converge slowly but with more stability.\n",
    "\n",
    "**Part 2: Optimizer Techniques**\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD):**\n",
    "   SGD updates parameters using gradients computed from a single training example, making it faster and more scalable than batch gradient descent. However, it may suffer from noisy updates and slower convergence due to its high variance.\n",
    "\n",
    "2. **Adam Optimizer:**\n",
    "   Adam optimizer combines momentum and adaptive learning rates. It maintains per-parameter learning rates and exponentially decaying moving averages of past gradients, providing faster convergence and better generalization. However, it requires more memory due to storing additional parameters.\n",
    "\n",
    "3. **RMSprop Optimizer:**\n",
    "   RMSprop addresses the challenges of adaptive learning rates by using a moving average of squared gradients to normalize the learning rates. It adapts the learning rates independently for each parameter, making it suitable for non-stationary objectives. RMSprop is computationally efficient and has shown robust performance in various tasks.\n",
    "\n",
    "**Part 3: Applying Optimizers**\n",
    "\n",
    "1. **Implementation and Comparison:**\n",
    "   Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a chosen framework. Train the model on a dataset and compare their impact on model convergence and performance metrics such as accuracy, loss, and training time.\n",
    "\n",
    "2. **Considerations in Choosing Optimizers:**\n",
    "   When choosing the appropriate optimizer for a neural network architecture and task, consider factors such as convergence speed, stability, generalization performance, and computational efficiency. Experiment with different optimizers and tune hyperparameters to find the optimal combination for the specific problem domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d56b8-7f4e-47a7-ac2e-43c1055c42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define deep learning model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with different optimizers\n",
    "sgd_optimizer = SGD(learning_rate=0.01)\n",
    "adam_optimizer = Adam(learning_rate=0.001)\n",
    "rmsprop_optimizer = RMSprop(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with SGD optimizer\n",
    "history_sgd = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Compile and train the model with Adam optimizer\n",
    "model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_adam = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Compile and train the model with RMSprop optimizer\n",
    "model.compile(optimizer=rmsprop_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_rmsprop = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate the models\n",
    "sgd_loss, sgd_accuracy = model.evaluate(X_test, y_test)\n",
    "adam_loss, adam_accuracy = model.evaluate(X_test, y_test)\n",
    "rmsprop_loss, rmsprop_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"SGD Test Loss:\", sgd_loss)\n",
    "print(\"SGD Test Accuracy:\", sgd_accuracy)\n",
    "\n",
    "print(\"Adam Test Loss:\", adam_loss)\n",
    "print(\"Adam Test Accuracy:\", adam_accuracy)\n",
    "\n",
    "print(\"RMSprop Test Loss:\", rmsprop_loss)\n",
    "print(\"RMSprop Test Accuracy:\", rmsprop_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
